{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"autosetup","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"uzY-4zyYvO6b"},"source":["*by vitor vilas-boas*\n","\n","*Biomedical Signal Processing*\n","\n","*Brain-Computer Interfaces*"]},{"cell_type":"code","metadata":{"id":"EdEFncxFEBnu"},"source":["!pip install mne\n","!pip install hyperopt==0.2.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m3RlzSM17Fk8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637233741767,"user_tz":180,"elapsed":44133,"user":{"displayName":"Vitor Mendes Vilas Boas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVeudX2oPgAupo54GDw7NRNYE8QJYonW1ja0nYpg=s64","userId":"15784608515749489606"}},"outputId":"3a7be01a-a777-476f-b3bd-9fd4a75cfb8c"},"source":["import os\n","import mne\n","import pickle\n","import warnings\n","import itertools\n","import numpy as np\n","import pandas as pd\n","from time import time\n","import seaborn as sns\n","from sklearn.svm import SVC\n","from scipy.io import loadmat\n","from scipy.stats import norm\n","from functools import partial\n","from scipy.linalg import eigh\n","from scipy.fftpack import fft\n","import matplotlib.pyplot as plt\n","from hyperopt import base, fmin, tpe, hp\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.preprocessing import normalize\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from scipy.signal import lfilter, butter, filtfilt, firwin, iirfilter, decimate, welch\n","from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, StratifiedKFold\n","from hyperopt.plotting import main_plot_history, main_plot_histogram, main_plot_vars, main_plot_1D_attachment\n","from sklearn.metrics import accuracy_score, cohen_kappa_score, classification_report, make_scorer, precision_recall_fscore_support, confusion_matrix\n","\n","np.seterr(divide='ignore', invalid='ignore')\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","mne.set_log_level(50, 50)\n","\n","def extractEpochs(data, events, smin, smax, class_ids):\n","    events_list = events[:, 1] # get class labels column\n","    cond = False\n","    for i in range(len(class_ids)): cond += (events_list == class_ids[i]) #get only class_ids pos in events_list\n","    idx = np.where(cond)[0]\n","    s0 = events[idx, 0] # get initial timestamps of each class epochs\n","    sBegin = s0 + smin\n","    sEnd = s0 + smax\n","    n_epochs = len(sBegin)\n","    n_channels = data.shape[0]\n","    n_samples = smax - smin\n","    epochs = np.zeros([n_epochs, n_channels, n_samples])\n","    labels = events_list[idx]\n","    bad_epoch_list = []\n","    for i in range(n_epochs):\n","        epoch = data[:, sBegin[i]:sEnd[i]]\n","        if epoch.shape[1] == n_samples: epochs[i, :, :] = epoch # Check if epoch is complete\n","        else:\n","            print('Incomplete epoch detected...', n_samples, '!=', epoch.shape[1])\n","            bad_epoch_list.append(i)\n","    labels = np.delete(labels, bad_epoch_list)\n","    epochs = np.delete(epochs, bad_epoch_list, axis=0)\n","    return epochs, labels\n","    \n","    \n","def nanCleaner(epoch):\n","    \"\"\" Removes NaN from data by interpolation\n","        data_in : input data - np matrix channels x samples\n","        data_out : clean dataset with no NaN samples \n","    \"\"\"\n","    for i in range(epoch.shape[0]):\n","        bad_idx = np.isnan(epoch[i, :])\n","        epoch[i, bad_idx] = np.interp(bad_idx.nonzero()[0], (~bad_idx).nonzero()[0], epoch[i, ~bad_idx])\n","    return epoch\n","    \n","    \n","def corrigeNaN(data):\n","    for ch in range(data.shape[0] - 1):\n","        this_chan = data[ch]\n","        data[ch] = np.where(this_chan == np.min(this_chan), np.nan, this_chan)\n","        mask = np.isnan(data[ch])\n","        meanChannel = np.nanmean(data[ch])\n","        data[ch, mask] = meanChannel\n","    return data\n","\n","\n","class Filter:\n","    def __init__(self, fl, fh, Fs, filtering, band_type='bandpass'):\n","        self.ftype = filtering['design']\n","        if self.ftype != 'DFT':\n","            nyq = 0.5*Fs\n","            low = fl/nyq\n","            high = fh/nyq        \n","            if low == 0: low = 0.001\n","            if high >= 1: high = 0.99\n","            if self.ftype == 'IIR':\n","                # self.b, self.a = iirfilter(filtering['iir_order'], [low, high], btype='band')\n","                self.b, self.a = butter(filtering['iir_order'], [low, high], btype=band_type)\n","            elif self.ftype == 'FIR':\n","                self.b, self.a = firwin(filtering['fir_order'], [low, high], window='hamming', pass_zero=False), [1]\n","\n","    def apply_filter(self, X, is_epoch=False):\n","        if self.ftype != 'DFT': XF = lfilter(self.b, self.a, X) # lfilter, filtfilt\n","        else:\n","            XF = fft(X)\n","            if is_epoch:\n","                real, imag = np.real(XF).T, np.imag(XF).T\n","                XF = np.transpose(list(itertools.chain.from_iterable(zip(imag, real))))\n","            else:\n","                real = np.transpose(np.real(XF), (2, 0, 1))\n","                imag = np.transpose(np.imag(XF), (2, 0, 1))\n","                XF = np.transpose(list(itertools.chain.from_iterable(zip(imag, real))), (1, 2, 0)) \n","        return XF\n","\n","\n","class CSP:\n","    def __init__(self, n_components=4):\n","        self.n_components = n_components\n","        self.filters_ = None\n","    def fit(self, X, t):\n","        ch = X.shape[1]\n","        class_ids = np.unique(t)\n","        X1 = X[class_ids[0] == t]\n","        X2 = X[class_ids[1] == t]\n","        S1, S2 = np.zeros((ch, ch)), np.zeros((ch, ch))\n","        for i in range(len(X1)): S1 += np.dot(X1[i], X1[i].T) / X1[i].shape[-1] # cov X[i]\n","        for i in range(len(X2)): S2 += np.dot(X2[i], X2[i].T) / X2[i].shape[-1] # ...sum((X*X.T)/q)\n","        S1 /= len(X1);\n","        S2 /= len(X2)\n","        [D, W] = eigh(S1, S1 + S2) # + 1e-10 * np.eye(22))\n","        ind = np.empty(ch, dtype=int)\n","        ind[0::2] = np.arange(ch-1, ch//2 - 1, -1)\n","        ind[1::2] = np.arange(0, ch//2)\n","        # W += 1e-1 * np.eye(22)\n","        W = W[:, ind]\n","        self.filters_ = W.T[:self.n_components]\n","        return self # used on cross-validation pipeline\n","    def transform(self, X):\n","        Y = np.asarray([np.dot(self.filters_, ep) for ep in X])\n","        # FEAT = np.log(np.mean(Y**2, axis=2))\n","        FEAT = np.log(np.var(Y, axis=2))\n","        return FEAT\n","\n","\n","def design_clf(clf_details):\n","    if clf_details['model'] == 'LDA': clf = LDA()\n","    if clf_details['model'] == 'LR':  clf = LogisticRegression(verbose=False)\n","    if clf_details['model'] == 'SVM':\n","        clf = SVC(kernel=clf_details['kernel']['kf'], C=10**(clf_details['C']), probability=True)\n","    if clf_details['model'] == 'KNN':\n","        clf = KNeighborsClassifier(n_neighbors=int(clf_details['neig'])) #, metric=clf_details['metric'], p=3)\n","    if clf_details['model'] == 'MLP':\n","        clf = MLPClassifier(verbose=False, max_iter=10000, learning_rate_init=10 ** clf_details['eta'], activation='tanh', # clf_details['activ']['af'], \n","                                 hidden_layer_sizes=(int(clf_details['n_neurons']), int(clf_details['n_hidden'])))\n","                                 # learning_rate='constant', solver=clf_details['mlp_solver']) # alpha=10**clf_details['alpha'], learning_rate=clf_details['eta_type'],\n","        clf.out_activation = 'softmax'                    \n","    if clf_details['model'] == 'Bayes': clf = GaussianNB()\n","    if clf_details['model'] == 'DTree': clf = DecisionTreeClassifier(criterion=clf_details['crit'], random_state=0)\n","    return clf\n","    \n","\n","def objective_func(args, data1, data2, events1, events2, path_eeg, class_ids=[1,2], Fs=250, filtering={'design':'DFT'},\n","                   crossval=True, nfolds=5, only_train=False, entropy_cost=False):\n","    # print(args)\n","    fl, fh, tmin, tmax, ncsp, nbands, clf_details = args\n","    \n","    clf = design_clf(clf_details)\n","    args = (class_ids, int(fl), int(fh), Fs, int(ncsp), int(nbands), filtering, clf)\n","    \n","    while(tmax-tmin) < 1: tmax += 0.5\n","    smin, smax = int(tmin*Fs), int(tmax*Fs)\n","    \n","    lb_utils = [7,8,9,10]\n","    \n","    ZT, tt = extractEpochs(data1, events1, smin, smax, [7,8,9,10])\n","    for i,k in zip(lb_utils, range(1, 5)): tt = np.where(tt == i, k, tt)\n","    ZT = np.vstack([ ZT[np.where(tt == k)] for k in class_ids ])\n","    tt = np.hstack([ np.ones(len(ZT)//2)*k for k in class_ids ]).astype(int) \n","    \n","    ZV, _ = extractEpochs(data2, events2, smin, smax, [7])\n","    tv = np.ravel(loadmat(path_eeg + 'true_labels/A0' + str(suj) + 'E.mat')['classlabel'])\n","    ZV = np.vstack([ ZV[np.where(tv == k)] for k in class_ids ])\n","    tv = np.hstack([ np.ones(len(ZV)//2)*k for k in class_ids ]).astype(int) \n","    \n","    ZT, ZV = nanCleaner(ZT), nanCleaner(ZV)\n","    \n","    if crossval:\n","        kf = StratifiedShuffleSplit(nfolds, test_size=1/nfolds, random_state=None)  \n","        # kf = StratifiedKFold(nfolds, False, random_state=None)\n","        if only_train: Z, t = np.copy(ZT), np.copy(tt)\n","        else: Z, t = np.vstack([ZT, ZV]), np.hstack([tt, tv])\n","        \n","        cross_acc, cross_kpa, cross_entropy = [], [], []\n","        for train, test in kf.split(Z, t):\n","            ZT, tt, ZV, tv = Z[train], t[train], Z[test], t[test]\n","            acc_fold, kpa_fold, entropy_fold = sbcsp_chain(ZT, ZV, tt, tv, args) if (nbands > 1) else classic_chain(ZT, ZV, tt, tv, args)\n","            cross_acc.append(acc_fold)\n","            cross_kpa.append(kpa_fold)\n","            cross_entropy.append(entropy_fold)\n","        acc, kpa, entropy = np.mean(cross_acc), np.mean(cross_kpa), np.mean(cross_entropy)\n","    \n","    else: acc, kpa, entropy = sbcsp_chain(ZT, ZV, tt, tv, args) if (nbands > 1) else classic_chain(ZT, ZV, tt, tv, args)\n","\n","    # Diferentemente da acurácia o erro por entropia cruzada já é um valor negativo, não sendo necessário multiplicar por (-1) antes de minimizar usando BO\n","    return entropy if entropy_cost else acc*(-1)\n","\n","\n","### Função de custo por entropia cruzada - alternativa ao erro da acurácia na busca pelo conjunto ótimo de hiperparâmetros \n","def crossEntropy(y, t):\n","    #y = np.where(y > 1, 1, y)\n","    #y = np.where(y < 0, 0, y)\n","    #E = 0 \n","    #for yn,tn in zip(y,t): E += (tn * np.log(yn)) + (1 - tn) * np.log(1 - yn)\n","    #return E\n","    return sum([(tn * np.log(yn)) + (1 - tn) * np.log(1 - yn) for yn,tn in zip(y,t)])\n","\n","\n","def classic_chain(ZT, ZV, tt, tv, args):\n","    # print('BU')\n","    class_ids, fl, fh, Fs, ncsp, nbands, filtering, clf = args\n","    filt = Filter(fl, fh, Fs, filtering)\n","    XT = filt.apply_filter(ZT)\n","    XV = filt.apply_filter(ZV)\n","    if filtering['design'] == 'DFT':  # extrai somente os bins referentes à banda de interesse\n","        bsize = 2/(Fs/ZT.shape[-1])  # 2==sen/cos do comp complexo da fft intercalados / resol freq \n","        XT = XT[:, :, round(fl * bsize):round(fh * bsize)]\n","        bsize = 2/(Fs/ZV.shape[-1])  \n","        XV = XV[:, :, round(fl * bsize):round(fh * bsize)]\n","    \n","    # TRAINING \n","    csp = CSP(n_components=ncsp)  # mne.decoding.CSP(...)\n","    csp.fit(XT, tt)\n","    csp_filters = csp.filters_\n","    FT = csp.transform(XT)\n","    clf.fit(FT, tt)\n","    \n","    # EVALUATE\n","    FV = csp.transform(XV)\n","    y, yp = clf.predict(FV), clf.predict_proba(FV)\n","    acc, kpa = np.mean(y == tv), cohen_kappa_score(y, tv)\n","\n","    # Função Custo por Entropia Cruzada - usar a primeira coluna de yp sendo 0.5 o limiar que define classe A (0) ou B (1), mesma saída de y\n","    entropy = crossEntropy(yp[:,0], tv)\n","    \n","    return acc, kpa, entropy\n","\n","\n","def sbcsp_chain(ZT, ZV, tt, tv, args):\n","    # print('SBCSP')\n","    class_ids, fl, fh, Fs, ncsp, nbands, filtering, clf = args\n","    \n","    if nbands > (fh-fl): nbands = int(fh-fl)\n","    step = (fh-fl) / (nbands+1) # n_bins/nbands+1\n","    size = step / 0.5 # step/overlap\n","    sub_bands = []\n","    for i in range(nbands):\n","        fl_sb = i * step + fl\n","        fh_sb = i * step + size + fl\n","        sub_bands.append([fl_sb, fh_sb])\n","    \n","    XT, XV = [], []\n","    if filtering['design'] == 'DFT':\n","        filt = Filter(fl, fh, Fs, filtering)\n","        XTF = filt.apply_filter(ZT)\n","        XVF = filt.apply_filter(ZV)\n","        for i in range(nbands):\n","            bsize = 2/(Fs/ZT.shape[-1]) # 2==sen/cos complexo fft intercalados / resol freq\n","            XT.append(XTF[:, :, round(sub_bands[i][0]*bsize):round(sub_bands[i][1]*bsize)])\n","            bsize = 2/(Fs/ZV.shape[-1])\n","            XV.append(XVF[:, :, round(sub_bands[i][0]*bsize):round(sub_bands[i][1]*bsize)])         \n","    elif filtering['design'] == 'IIR':\n","        for i in range(nbands):\n","            filt = Filter(sub_bands[i][0], sub_bands[i][1], Fs, filtering)\n","            XT.append(filt.apply_filter(ZT))\n","            XV.append(filt.apply_filter(ZV))\n","    \n","    # TRAINING        \n","    csp = [ CSP(n_components=ncsp) for i in range(nbands) ] # mne.decoding.CSP()\n","    for i in range(nbands): csp[i].fit(XT[i], tt)\n","    csp_filters = [csp[i].filters_[:int(ncsp)] for i in range(nbands)]\n","\n","    FT = [ csp[i].transform(XT[i]) for i in range(nbands) ]\n","    ldas = [ LDA() for i in range(nbands) ]\n","    for i in range(nbands): ldas[i].fit(FT[i], tt)\n","    ST = np.asarray([ np.ravel(ldas[i].transform(FT[i])) for i in range(nbands)]).T # Score LDA\n","    \n","    p0 = norm(np.mean(ST[tt == class_ids[0], :], axis=0), np.std(ST[tt == class_ids[0], :], axis=0))\n","    p1 = norm(np.mean(ST[tt == class_ids[1], :], axis=0), np.std(ST[tt == class_ids[1], :], axis=0))\n","    META_ST = np.log(p0.pdf(ST) / p1.pdf(ST))\n","    clf.fit(META_ST, tt)\n","    \n","    # EVALUATE\n","    FV = [ csp[i].transform(XV[i]) for i in range(nbands) ]\n","    SV = np.asarray([ np.ravel(ldas[i].transform(FV[i])) for i in range(nbands)]).T\n","    META_SV = np.log(p0.pdf(SV) / p1.pdf(SV))\n","    y, yp = clf.predict(META_SV), clf.predict_proba(META_SV)\n","    acc, kpa = np.mean(y == tv), cohen_kappa_score(y, tv) # svm.score(META_SV, tv)\n","\n","    # Função Custo por Entropia Cruzada - usar a primeira coluna de yp sendo 0.5 o limiar que define classe A (0) ou B (1), mesma saída de y\n","    entropy = crossEntropy(yp[:,0], tv)\n","    \n","    # svm = SVC(kernel='linear', C=1e-4, probability=True)\n","    # svm.fit(META_ST, tt)\n","    # y, yp = svm.predict(META_SV), svm.predict_proba(META_SV); \n","    # acc_svm = round(np.mean(y == tv)*100,2) # round(svm.score(META_SV, tv)*100,2)\n","    # print('SVM acc:', acc_svm)\n","    \n","    # lda = LDA()\n","    # lda.fit(META_ST, tt)\n","    # y, yp = lda.predict(META_SV), lda.predict_proba(META_SV)\n","    # acc_lda = round(np.mean(y == tv)*100,2) # round(lda.score(META_SV, tv)*100,2)\n","    # print('LDA acc:', acc_lda)\n","    \n","    # TT, TV = np.zeros((len(ZT), 2)), np.zeros((len(ZV), 2))\n","    # for i in range(2): TT[:,i] = np.where(tt == i+1, 1, TT[:,i])\n","    # for i in range(2): TV[:,i] = np.where(tv == i+1, 1, TV[:,i])\n","    \n","    # mlp = MLPClassifier(hidden_layer_sizes=(100,2), max_iter=10000, activation='tanh', verbose=False) #, random_state=42)\n","    # mlp.out_activation = 'softmax' # 'logistic', 'softmax', # mlp.outputs = 3\n","    # mlp.fit(META_ST, TT)\n","    # Y, YP = mlp.predict(META_SV), mlp.predict_proba(META_SV)\n","    # y = np.argmax(YP, axis=1)+1\n","    # acc_mlp = round(np.mean(y == tv)*100,2) # round(mlp.score(META_SV, TV)*100,2)\n","    # print('MLP acc:', acc_mlp)\n","    \n","    return acc, kpa, entropy\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"VOTFh5-f7z4Q"},"source":["suj = 1\n","class_ids = [1,2]\n","ch=range(0,22) \n","Fs=250\n","path_eeg = '/content/drive/My Drive/BCI_workgroup/dataset/IV2a/'\n","path_AS_trials = '/content/drive/My Drive/BCI_workgroup/results/' + 'A0' + str(suj) + '_trials.pkl'\n","filtering = {'design':'DFT'}\n","# filtering = {'design':'IIR', 'iir_order':5}\n","\n","eeg = mne.io.read_raw_gdf(path_eeg + 'A0' + str(suj) + 'T.gdf').load_data()\n","data1 = eeg.get_data()[:22] # [channels x samples]\n","events1 = mne.events_from_annotations(eeg) # raw.find_edf_events()\n","ch_names = eeg.ch_names\n","\n","eeg = mne.io.read_raw_gdf(path_eeg + 'A0' + str(suj) + 'E.gdf').load_data()\n","data2 = eeg.get_data()[ch] # [channels x samples]\n","events2 = mne.events_from_annotations(eeg) # raw.find_edf_events()\n","\n","# for k,v in events1[1].items(): print(f'{k}:: {v}')\n","# for k,v in events2[1].items(): print(f'{k}:: {v}')\n","# lb_utils = [7,8,9,10]\n","events1 = np.delete(events1[0], 1, axis=1)\n","events2 = np.delete(events2[0], 1, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-XBPPBXEPrN","executionInfo":{"status":"ok","timestamp":1637185004263,"user_tz":180,"elapsed":209512,"user":{"displayName":"Vitor Mendes Vilas Boas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVeudX2oPgAupo54GDw7NRNYE8QJYonW1ja0nYpg=s64","userId":"15784608515749489606"}},"outputId":"761df831-f3b9-4706-9b26-92eb582d8868"},"source":["# =============================================================================\n","# AUTO SETUP    \n","# =============================================================================\n","niter = 100\n","space = (\n","    hp.uniformint('fl', 0, 15),\n","    hp.uniformint('fh', 25, 50),\n","    hp.quniform('tmin', 0, 2, 0.5),\n","    hp.quniform('tmax', 2, 4, 0.5),\n","    hp.choice('ncsp', [2, 4, 6, 8, len(ch)]),\n","    hp.uniformint('nbands', 1, 25),\n","    hp.choice('clf', [\n","        {'model':'LR'},\n","        {'model':'LDA'},\n","        {'model':'KNN', 'neig':hp.uniformint('neig', 2, 20)}, #'metric':hp.choice('metric', ['euclidean', 'manhattan', 'minkowski', 'chebyshev'])},\n","        {'model':'SVM', 'C':hp.quniform('C', -6, 0, 1), 'kernel':hp.choice('kernel', [{'kf':'linear'}, {'kf':'poly'}, {'kf':'sigmoid'}, {'kf':'rbf'}])},\n","        {'model':'MLP', 'eta':hp.quniform('eta', -5, -2, 1), 'n_neurons':hp.quniform('n_neurons', 20, 100, 20),  \n","         'n_hidden':hp.uniformint('n_hidden', 1, 2)}, # 'activ':hp.choice('activ', [{'af':'logistic'}, {'af':'tanh'}])}\n","        ]))\n","\n","trials = base.Trials()\n","try: trials = pickle.load(open(path_AS_trials, 'rb'))\n","except: trials = base.Trials()\n","try:\n","    print('Past trials: ' + str(len(trials)))\n","    # init_vals = [{'fl':4, 'fh':40, 'tmin':0.5, 'tmax':2.5, 'ncsp':8, 'nbands':9, 'clf':{'model':'SVM', 'C':-4, 'kernel':{'kf':'linear'}}}]\n","    init_vals = [{'fl':8, 'fh':30, 'tmin':0.5, 'tmax':2.5, 'ncsp':8, 'nbands':1, 'clf':{'model':'LDA'}}]\n","    obfunc = partial(objective_func, data1=data1, data2=data2, events1=events1, events2=events2, path_eeg=path_eeg, \n","                     class_ids=class_ids, Fs=Fs, filtering=filtering, crossval=True, nfolds=5, only_train=True, entropy_cost=True)\n","    best = fmin(obfunc, space=space, algo=tpe.suggest, max_evals=len(trials)+niter, trials=trials, points_to_evaluate=init_vals)\n","    pickle.dump(trials, open(path_AS_trials, 'wb'))\n","\n","    # drive = GoogleDrive(gauth)  \n","    # file = drive.CreateFile({'parents':[{u'id': '/content/drive/My Drive/BCI_workgroup/results/'}]})\n","    # file.SetContentFile('A0' + str(suj) + '_trials.pkl')\n","    # file.Upload()\n","except:\n","    pickle.dump(trials, open(path_AS_trials, 'wb'))\n","    raise\n","\n","# best = trials.best_trial['misc']['vals'] # trials.argmin\n","if best['clf'] == 0: clf = {'model':'LR'}\n","elif best['clf'] == 1: clf = {'model':'LDA'}\n","elif best['clf'] == 2: clf = {'model':'KNN', 'neig':best['neig']}\n","    # metric = 'euclidean' if best['metric']==0 else 'manhattan' if best['metric']==1 else 'minkowski' if best['metric']==2 else 'chebyshev' \n","    # clf = {'model':'KNN', 'metric':metric, 'neig':best['neig']}\n","elif best['clf'] == 3:\n","    kernel = 'linear' if best['kernel']==0 else 'poly' if best['kernel']==1 else 'sigmoid' if best['kernel']==2 else 'rbf'\n","    clf = {'model':'SVM', 'kernel':{'kf':kernel}, 'C':best['C']}\n","elif best['clf'] == 4: clf = {'model':'MLP', 'eta':best['eta'], 'n_neurons':best['n_neurons'], 'n_hidden':best['n_hidden']}\n","    # activ = 'logistic' if best['activ']==0 else 'tanh'\n","    # clf = {'model':'MLP', 'eta':best['eta'], 'activ':{'af':activ}, 'n_neurons':best['n_neurons'], 'n_hidden':best['n_hidden']}\n","ncsp = [2,4,6,8,len(ch)][best['ncsp']]\n","\n","args = (int(best['fl']), int(best['fh']), best['tmin'], best['tmax'], ncsp, int(best['nbands']), clf)\n","acc = objective_func(args, data1, data2, events1, events2, path_eeg, class_ids=class_ids, Fs=Fs, filtering=filtering, crossval=False)\n","print(f\"Setup: {args}\")\n","print('AS   Acc:', round(acc*100,2)*(-1))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Past trials: 100\n","100%|██████████| 200/200 [03:28<00:00,  2.08s/trial, best loss: -inf]\n","Setup: (4, 41, 0.5, 4.0, 8, 19, {'model': 'LDA'})\n","AS   Acc: 87.5\n"]}]},{"cell_type":"code","metadata":{"id":"W1h1wUmb_S-y"},"source":["# =============================================================================\n","# CMBU - Configuração Manual de banda Única   \n","# =============================================================================\n","# t0 = time()\n","filtering = {'design':'IIR', 'iir_order':5}\n","args = (8, 30, 0.5, 2.5, 8, 1, {'model':'LDA'})\n","acc = objective_func(args, data1, data2, events1, events2, path_eeg, class_ids=class_ids, Fs=Fs, filtering=filtering, crossval=False)\n","print('CMBU Acc:', round(acc*100,2)*(-1))\n","# print('runtime:', round(time()-t0,3), '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGLpKI3f_V9t","executionInfo":{"status":"ok","timestamp":1637176579466,"user_tz":180,"elapsed":1302,"user":{"displayName":"Vitor Mendes Vilas Boas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVeudX2oPgAupo54GDw7NRNYE8QJYonW1ja0nYpg=s64","userId":"15784608515749489606"}},"outputId":"f9859eb7-245b-452c-fa0f-712873220e78"},"source":["# =============================================================================\n","# CMSB    \n","# =============================================================================\n","filtering = {'design':'IIR', 'iir_order':5}\n","args = (4, 40, 0.5, 2.5, 8, 9, {'model':'SVM', 'C':-4, 'kernel':{'kf':'linear'}})\n","acc = objective_func(args, data1, data2, events1, events2, path_eeg, class_ids=class_ids, Fs=Fs, filtering=filtering, crossval=False)\n","print('CMSB Acc:', round(acc*100,2)*(-1))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CMSB Acc: 85.42\n"]}]},{"cell_type":"code","metadata":{"id":"PKwaHv-4J-8o"},"source":["# # =============================================================================\n","# # Analysis\n","# # =============================================================================\n","# trials = pickle.load(open(path_AS_trials, 'rb'))\n","# all_loss = [ trials.trials[i]['result']['loss'] * (-1) for i in range(len(trials.trials)) ]\n","# # main_plot_history(trials)\n","# # main_plot_histogram(trials)\n","# main_plot_vars(trials, do_show=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ov-Au-OiM_iF"},"source":["# x = np.linspace(0, len(x), len(x))\n","# y = trials.vals['nbands']\n","# plt.scatter(x, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"REaAwA2Zv7xw"},"source":[""],"execution_count":null,"outputs":[]}]}