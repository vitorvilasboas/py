{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sbcsp_clean.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"G0_SpmJpD8XP"},"source":["!pip install mne\n","import mne\n","import warnings\n","import itertools\n","import numpy as np\n","from time import time\n","from sklearn.svm import SVC\n","from scipy.io import loadmat\n","from scipy.stats import norm\n","from scipy.linalg import eigh\n","from scipy.fftpack import fft\n","from sklearn.preprocessing import normalize\n","from sklearn.metrics import confusion_matrix\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from scipy.signal import lfilter, butter, filtfilt, firwin, iirfilter, decimate, welch\n","\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","mne.set_log_level(50, 50)\n","\n","def extractEpochs(data, events, smin, smax, class_ids):\n","    events_list = events[:, 1] # get class labels column\n","    cond = False\n","    for i in range(len(class_ids)): cond += (events_list == class_ids[i]) #get only class_ids pos in events_list\n","    idx = np.where(cond)[0]\n","    s0 = events[idx, 0] # get initial timestamps of each class epochs\n","    sBegin = s0 + smin\n","    sEnd = s0 + smax\n","    n_epochs = len(sBegin)\n","    n_channels = data.shape[0]\n","    n_samples = smax - smin\n","    epochs = np.zeros([n_epochs, n_channels, n_samples])\n","    labels = events_list[idx]\n","    bad_epoch_list = []\n","    for i in range(n_epochs):\n","        epoch = data[:, sBegin[i]:sEnd[i]]\n","        if epoch.shape[1] == n_samples: epochs[i, :, :] = epoch # Check if epoch is complete\n","        else:\n","            print('Incomplete epoch detected...', n_samples, '!=', epoch.shape[1])\n","            bad_epoch_list.append(i)\n","    labels = np.delete(labels, bad_epoch_list)\n","    epochs = np.delete(epochs, bad_epoch_list, axis=0)\n","    return epochs, labels\n","    \n","def nanCleaner(epoch):\n","    \"\"\"Removes NaN from data by interpolation\n","    data_in : input data - np matrix channels x samples\n","    data_out : clean dataset with no NaN samples\"\"\"\n","    for i in range(epoch.shape[0]):\n","        bad_idx = np.isnan(epoch[i, :])\n","        epoch[i, bad_idx] = np.interp(bad_idx.nonzero()[0], (~bad_idx).nonzero()[0], epoch[i, ~bad_idx])\n","    return epoch\n","    \n","def corrigeNaN(data):\n","    for ch in range(data.shape[0] - 1):\n","        this_chan = data[ch]\n","        data[ch] = np.where(this_chan == np.min(this_chan), np.nan, this_chan)\n","        mask = np.isnan(data[ch])\n","        meanChannel = np.nanmean(data[ch])\n","        data[ch, mask] = meanChannel\n","    return data\n","\n","\n","class Filter:\n","    def __init__(self, fl, fh, Fs, filtering, band_type='bandpass'):\n","        self.ftype = filtering['design']\n","        if self.ftype != 'DFT':\n","            nyq = 0.5*Fs\n","            low = fl/nyq\n","            high = fh/nyq        \n","            if low == 0: low = 0.001\n","            if high >= 1: high = 0.99\n","            if self.ftype == 'IIR':\n","                # self.b, self.a = iirfilter(filtering['iir_order'], [low, high], btype='band')\n","                self.b, self.a = butter(filtering['iir_order'], [low, high], btype=band_type)\n","            elif self.ftype == 'FIR':\n","                self.b, self.a = firwin(filtering['fir_order'], [low, high], window='hamming', pass_zero=False), [1]\n","\n","    def apply_filter(self, X, is_epoch=False):\n","        if self.ftype != 'DFT': XF = lfilter(self.b, self.a, X) # lfilter, filtfilt\n","        else:\n","            XF = fft(X)\n","            if is_epoch:\n","                real, imag = np.real(XF).T, np.imag(XF).T\n","                XF = np.transpose(list(itertools.chain.from_iterable(zip(imag, real))))\n","            else:\n","                real = np.transpose(np.real(XF), (2, 0, 1))\n","                imag = np.transpose(np.imag(XF), (2, 0, 1))\n","                XF = np.transpose(list(itertools.chain.from_iterable(zip(imag, real))), (1, 2, 0)) \n","        return XF\n","\n","class CSP:\n","    def __init__(self, n_components):\n","        self.n_components = n_components\n","        self.filters_ = None\n","    def fit(self, X, t):\n","        ch = X.shape[1]\n","        class_ids = np.unique(t)   \n","        X1 = X[class_ids[0] == t]\n","        X2 = X[class_ids[1] == t]\n","        S1, S2 = np.zeros((ch, ch)), np.zeros((ch, ch))  \n","        for i in range(len(X1)): S1 += np.dot(X1[i], X1[i].T) / X1[i].shape[-1] # cov X[i]\n","        for i in range(len(X2)): S2 += np.dot(X2[i], X2[i].T) / X2[i].shape[-1] # ...sum((X*X.T)/q)\n","        S1 /= len(X1); \n","        S2 /= len(X2)\n","        [D, W] = eigh(S1, S1 + S2) # + 1e-10 * np.eye(22))\n","        ind = np.empty(ch, dtype=int)\n","        ind[0::2] = np.arange(ch-1, ch//2 - 1, -1) \n","        ind[1::2] = np.arange(0, ch//2)\n","        # W += 1e-1 * np.eye(22)\n","        W = W[:, ind]\n","        self.filters_ = W.T[:self.n_components]\n","        return self # used on cross-validation pipeline\n","    def transform(self, X):        \n","        Y = np.asarray([np.dot(self.filters_, ep) for ep in X])\n","        # FEAT = np.log(np.mean(Y**2, axis=2))\n","        FEAT = np.log(np.var(Y, axis=2))\n","        return FEAT\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oyKbXKKXHRwy"},"source":["eeg = mne.io.read_raw_gdf('/content/drive/My Drive/BCI_workgroup/dataset/IV2a/A02T.gdf').load_data()\n","data1 = eeg.get_data()[:22] # [channels x samples]\n","events1 = mne.events_from_annotations(eeg) # raw.find_edf_events()\n","ch_names = eeg.ch_names\n","\n","eeg = mne.io.read_raw_gdf('/content/drive/My Drive/BCI_workgroup/dataset/IV2a/A02E.gdf').load_data()\n","data2 = eeg.get_data()[:22] # [channels x samples]\n","events2 = mne.events_from_annotations(eeg) # raw.find_edf_events()\n","\n","for k,v in events1[1].items(): print(f'{k}:: {v}')\n","print(\"-------------------\")\n","for k,v in events2[1].items(): print(f'{k}:: {v}')\n","# lb_utils = [4,5,6,7]   # <-------- VOCÊ PRECISA VERIFICAR QUAIS SÃO OS LABELS QUE O MNE ATRIBUIU (depende da versão do MNE)\n","lb_utils = [7,8,9,10]    # <-------- conforme pode observar no print abaixo, os labels equivalentes às 4 classes no protocolo experimental \n","                         #           (VIDE BRUNNER et al., 2008) são 7,8,9,10 no dataset de treinamento e 7 no de validação\n","\n","events1 = np.delete(events1[0], 1, axis=1)\n","events2 = np.delete(events2[0], 1, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DLaspAjvEZdD"},"source":["class_ids = [1,2]\n","tmin, tmax, fl, fh, ncsp, nbands = 0.5, 2.5, 4, 40, 8, 9\n","filtering = {'design':'DFT'} \n","# filtering = {'design':'IIR', 'iir_order':5}\n","\n","Fs = 250\n","smin, smax = int(tmin*Fs), int(tmax*Fs)\n","\n","ZT, tt = extractEpochs(data1, events1, smin, smax, lb_utils)\n","for i,k in zip(lb_utils, range(1, 5)): tt = np.where(tt == i, k, tt)\n","ZT = np.vstack([ ZT[np.where(tt == k)] for k in class_ids ])\n","tt = np.hstack([ np.ones(len(ZT)//2)*k for k in class_ids ]).astype(int) \n","\n","# ZV, _ = extractEpochs(data2, events2, smin, smax, [4])\n","ZV, _ = extractEpochs(data2, events2, smin, smax, [7]) # <--------- Aqui, portanto, o argumento passa a ser 7 e não 4\n","tv = np.ravel(loadmat('/content/drive/My Drive/BCI_workgroup/dataset/IV2a/true_labels/A02E.mat')['classlabel'])\n","ZV = np.vstack([ ZV[np.where(tv == k)] for k in class_ids ])\n","tv = np.hstack([ np.ones(len(ZV)//2)*k for k in class_ids ]).astype(int) \n","\n","t0 = time()\n","step = (fh-fl) / (nbands+1) # n_bins/nbands+1\n","size = step / 0.5 # step/overlap\n","sub_bands = []\n","for i in range(nbands):\n","    fl_sb = i * step + fl\n","    fh_sb = i * step + size + fl\n","    sub_bands.append([fl_sb, fh_sb])\n","\n","XT, XV = [], []\n","if filtering['design'] == 'DFT':\n","    filt = Filter(fl, fh, Fs, filtering)\n","    XTF = filt.apply_filter(ZT)\n","    XVF = filt.apply_filter(ZV)\n","    # XTF = filt.apply_filter(ZV)\n","    # XVF = filt.apply_filter(ZT)\n","    for i in range(nbands):\n","        bsize = 2/(Fs/ZT.shape[-1]) # 2 == sen/cos\n","        XT.append(XTF[:, :, round(sub_bands[i][0]*bsize):round(sub_bands[i][1]*bsize)])\n","        bsize = 2/(Fs/ZV.shape[-1])\n","        XV.append(XVF[:, :, round(sub_bands[i][0]*bsize):round(sub_bands[i][1]*bsize)])         \n","elif filtering['design'] == 'IIR':\n","    for i in range(nbands):\n","        filt = Filter(sub_bands[i][0], sub_bands[i][1], Fs, filtering)\n","        XT.append(filt.apply_filter(ZT))\n","        XV.append(filt.apply_filter(ZV))\n","        \n","csp = [ CSP(n_components=ncsp) for i in range(nbands) ] # mne.decoding.CSP()\n","for i in range(nbands): csp[i].fit(XT[i], tt)\n","FT = [ csp[i].transform(XT[i]) for i in range(nbands) ]\n","FV = [ csp[i].transform(XV[i]) for i in range(nbands) ]\n","\n","ldas = [ LDA() for i in range(nbands) ]\n","for i in range(nbands): ldas[i].fit(FT[i], tt)\n","ST = np.asarray([ np.ravel(ldas[i].transform(FT[i])) for i in range(nbands)]).T # Score LDA\n","SV = np.asarray([ np.ravel(ldas[i].transform(FV[i])) for i in range(nbands)]).T \n","    \n","p0 = norm(np.mean(ST[tt == class_ids[0], :], axis=0), np.std(ST[tt == class_ids[0], :], axis=0))\n","p1 = norm(np.mean(ST[tt == class_ids[1], :], axis=0), np.std(ST[tt == class_ids[1], :], axis=0))\n","META_ST = np.log(p0.pdf(ST) / p1.pdf(ST))\n","META_SV = np.log(p0.pdf(SV) / p1.pdf(SV))\n","\n","svm = SVC(kernel='linear', C=1e-4, probability=True)\n","svm.fit(META_ST, tt)\n","y, yp = svm.predict(META_SV), svm.predict_proba(META_SV); \n","acc_svm = round(np.mean(y == tv)*100,2) # round(svm.score(META_SV, tv)*100,2)\n","print('SVM acc:', acc_svm)\n","\n","lda = LDA()\n","lda.fit(META_ST, tt)\n","y, yp = lda.predict(META_SV), lda.predict_proba(META_SV)\n","acc_lda = round(np.mean(y == tv)*100,2) # round(lda.score(META_SV, tv)*100,2)\n","print('LDA acc:', acc_lda)\n","\n","TT, TV = np.zeros((len(ZT), 2)), np.zeros((len(ZV), 2))\n","for i in range(2): TT[:,i] = np.where(tt == i+1, 1, TT[:,i])\n","for i in range(2): TV[:,i] = np.where(tv == i+1, 1, TV[:,i])\n","\n","mlp = MLPClassifier(hidden_layer_sizes=(100,2), max_iter=10000, activation='tanh', verbose=False) #, random_state=42)\n","mlp.out_activation = 'softmax' # 'logistic', 'softmax', # mlp.outputs = 3\n","mlp.fit(META_ST, TT)\n","Y, YP = mlp.predict(META_SV), mlp.predict_proba(META_SV)\n","y = np.argmax(YP, axis=1)+1\n","acc_mlp = round(np.mean(y == tv)*100,2) # round(mlp.score(META_SV, TV)*100,2)\n","print('MLP acc:', acc_mlp)\n","\n","print('runtime:', round(time()-t0,3), '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gviMNmYPMhI8"},"source":[""],"execution_count":null,"outputs":[]}]}